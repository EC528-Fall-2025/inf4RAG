verbose: True

description: "### This is a chatbot interface powered by vLLM"

examples:
- How are you?
- Is it possible to build a room temperature superconductor?
- Should I go to the beach in Santa Cruz tomorrow?
- Explain the conflict between Zuck and Elon
- Summarize today's nytimes.com headlines

enabled_models:
- vLLM

enabled_browsers:
- Chrome

# LLM / model server (vLLM)
# Point this to your vLLM or OpenAI-compatible endpoint.
openstack_ip_port: "199.94.61.249:8000"
api_key: "ec528"

# Optional: explicit default model name.
# If empty, we still try to auto-detect from /v1/models.
default_model: "qwen-4b-instruct"

# RAG backend (Flask service)
# This should be the base URL WITHOUT the /rag suffix.
rag_base: "http://199.94.61.249:8001"
rag_upload_field: "file"

temperature: 0.1

# Agent max steps; used as AGENT_MAX_STEPS in app.py
max_actions: 5

