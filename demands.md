Optimizing Cloud-Based Inference for RAG and Agentic Workloads

Project Name: Optimizing Cloud-Based Inference for RAG and Agentic Workloads
Project Proposer: (Name, Email): Shripad Nadgowda (Shripad.nadgowda@intel.com)
Open Source: (Yes/No): Yes
Mentors: (Name, Email) [Add more if needed]: Shripad Nadgowda
Preferred Experience:
List technical skills and rank each as:

Required (for most or all team members): Python, Linux Shell
Required (at least one team member): Docker, Kubernetes, Cloud Platforms
Valuable: LLMs, ML frameworks
Nice to have: vLLM, Benchmarking GPU accelerators

Project Background:
In this project, we will explore the design and deployment of AI inference stacks on the cloud. The focus will be on understanding how modern inference workloads, such as Retrieval-Augmented Generation (RAG) and agentic workflows can be efficiently served using cloud-native infrastructure. We will evaluate different components of the inference stack, including model serving frameworks, orchestration layers, caching strategies, and GPU/accelerator utilization. A key objective is to identify performance and cost trade-offs when deploying these systems on major cloud platforms. By the end of the project, students will deliver a report benchmarking various inference stack configurations and highlighting best practices for scalable, high-performance deployment.

Project Description:
For this project, we will leverage Intel's Gaudi accelerators (or other available GPU resources) to conduct a series of experiments on cloud-based AI inference workloads. The core inference engine will be built using vLLM, with orchestration and control managed via llm-d or equivalent frameworks. We will test a variety of open-source large language models (LLMs) for representative inference tasks, including text generation, question answering, and retrieval-augmented workflows. To ensure a rigorous evaluation, we will design and implement a standardized benchmarking suite that measures key performance metrics such as throughput, latency, cost-efficiency, and resource utilization across different configurations of the inference stack. The outcome will be a comparative analysis that informs best practices for deploying scalable and efficient inference systems on the cloud

Learning Outcomes:
By the end of this project, students will be able to:

        •        Understand AI inference architectures and how modern LLM workloads (e.g., RAG, agentic tasks) are structured and executed in cloud environments.

        •        Deploy and manage inference stacks using tools such as vLLM and llm-d, and integrate them with open-source language models.

        •        Configure and benchmark accelerator-based infrastructure (e.g., Intel Gaudi, NVIDIA GPUs) for high-performance model serving.

        •        Design and run benchmarking experiments to evaluate inference performance in terms of latency, throughput, scalability, and cost.

