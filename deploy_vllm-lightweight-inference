# 
kubectl delete deployment vllm-lightweight -n vllm-inference
kubectl delete service vllm-lightweight-service -n vllm-inference

# 
kubectl get pods -n vllm-inference
kubectl get services -n vllm-inference

# don't need to do this
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpt2-simple
  namespace: vllm-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gpt2-simple
  template:
    metadata:
      labels:
        app: gpt2-simple
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: t3.large
      containers:
      - name: gpt2
        image: python:3.9-slim
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2"
        command: ["/bin/bash"]
        args:
        - -c
        - |
          pip install transformers torch flask
          cat > app.py << 'EOL'
          from flask import Flask, request, jsonify
          from transformers import GPT2LMHeadModel, GPT2Tokenizer
          import torch
          
          app = Flask(__name__)
          model_name = "gpt2"
          tokenizer = GPT2Tokenizer.from_pretrained(model_name)
          model = GPT2LMHeadModel.from_pretrained(model_name)
          
          @app.route('/health', methods=['GET'])
          def health():
              return jsonify({"status": "healthy"})
          
          @app.route('/v1/models', methods=['GET'])
          def models():
              return jsonify({
                  "object": "list",
                  "data": [{"id": "gpt2", "object": "model"}]
              })
          
          @app.route('/v1/completions', methods=['POST'])
          def completions():
              data = request.json
              prompt = data.get('prompt', '')
              max_tokens = data.get('max_tokens', 50)
              
              inputs = tokenizer.encode(prompt, return_tensors='pt')
              with torch.no_grad():
                  outputs = model.generate(inputs, max_length=inputs.shape[1] + max_tokens, num_return_sequences=1)
              
              generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
              response_text = generated_text[len(prompt):]
              
              return jsonify({
                  "choices": [{
                      "text": response_text,
                      "finish_reason": "length"
                  }]
              })
          
          if __name__ == '__main__':
              app.run(host='0.0.0.0', port=8000)
          EOL
          python app.py
---
apiVersion: v1
kind: Service
metadata:
  name: gpt2-simple-service
  namespace: vllm-inference
spec:
  selector:
    app: gpt2-simple
  ports:
  - port: 8000
    targetPort: 8000
  type: LoadBalancer
EOF

# 
kubectl get pods -n vllm-inference
kubectl get services -n vllm-inference

# 
curl http://a00cb1042dc104a85a26976f413de0c6-1647472312.us-west-2.elb.amazonaws.com:8000/health

kubectl port-forward service/gpt2-simple-service 8000:8000 -n vllm-inference

# another terminal test
curl http://localhost:8000/health

curl http://localhost:8000/v1/models

curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt2",
    "prompt": "Hello, world!",
    "max_tokens": 50
  }'
